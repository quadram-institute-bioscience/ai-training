---
layout: default
title: pexplexity curated list
---
<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

> Generated by perplexity

# Enhanced Large Language Model (LLM) Learning & Tooling Roadmap

The original reading list already covers many foundational and applied topics. The following extended roadmap deepens coverage across five critical dimensions—alignment & fine-tuning, retrieval-augmented generation, evaluation & benchmarking, scalable deployment, and professional upskilling—ensuring learners can progress from novice to production-grade practitioner.

## Alignment, Fine-Tuning & Parameter-Efficient Techniques

Modern LLM workflows rarely stop at pre-trained checkpoints. Alignment methods such as instruction tuning, reinforcement learning from human feedback (RLHF), and parameter-efficient fine-tuning (PEFT) unlock domain-specific, user-aligned behaviour.

1. **RLHF Foundations**
    - V7 Labs’ plain-language explainer walks through each stage of the RLHF pipeline—from supervised fine-tuning (SFT) through reward modelling to PPO training—giving newcomers an [end-to-end conceptual map](https://www.v7labs.com/blog/rlhf-reinforcement-learning-from-human-feedback).
    - Hugging Face’s “Illustrating RLHF” blog formalises the process and provides minimal code snippets using the [`trl` library](https://huggingface.co/blog/rlhf).
    - ICML 2023’s full-day tutorial materials (slides + code) supply lecture-quality depth plus exercises on preference data collection and [reward shaping](https://icml.cc/virtual/2023/tutorial/21554).
2. **Scalable RLHF Frameworks**
    - The OpenRLHF research paper details a Ray-based multi-node scheduler that fits >70 B-parameter models by decoupling policy, reward, reference and [critic networks](https://huggingface.co/papers/2405.11143).
    - `trl + peft` sample scripts demonstrate how to align 20 B-parameter Llama-2 variants on a single RTX 4090 via [LoRA adapters](https://huggingface.co/blog/trl-peft).
3. **Instruction & SFT Datasets**
    - WandB’s two-part series on Alpaca-style data preparation shows tokenisation, EOS handling and packing strategies for [long-context training](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2).
    - IBM’s overview of instruction tuning contrasts template-based SFT with pure RL approaches and summarises the FLAN ablation study on [zero-shot generalisation](https://www.ibm.com/think/topics/instruction-tuning).
4. **Parameter-Efficient Fine-Tuning (PEFT)**
    - Torchtune’s Llama-2 LoRA tutorial provides GPU memory arithmetic, hyper-parameter heuristics and [ablation plots](https://docs.pytorch.org/torchtune/0.4/tutorials/lora_finetune.html).
    - Sebastian Raschka’s “Practical Tips for LoRA” article benchmarks QLoRA, full-matrix LoRA and selective layer training across hundreds of runs, highlighting optimiser and [epoch trade-offs](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms).
    - Hugging Face PEFT docs consolidate adapter methods—LoRA, IA3, AdaLoRA, prefix tuning—with ready-to-use [`LoraConfig` code blocks](https://huggingface.co/docs/transformers/en/peft).
    - Hopsworks’ PEFT explainer quantifies memory savings (65 B model on 48 GB GPU) and contrasts LoRA vs [QLoRA deployments](https://www.hopsworks.ai/dictionary/parameter-efficient-fine-tuning-of-llms).

## Retrieval-Augmented Generation & Agentic Systems

After alignment, retrieval-augmented generation (RAG) remains the dominant technique for grounding models in up-to-date or proprietary data.

1. **Core RAG Tutorials**
    - LangChain’s two-part textual tutorial implements indexing, splitter selection, vector storage and [retrieval-then-generate chains](https://python.langchain.com/docs/tutorials/rag/), then extends to conversational memory with [`MessagesState`](https://python.langchain.com/docs/tutorials/qa_chat_history/).
    - Haystack 2.0’s nine-minute step-through video pairs SentenceTransformers embeddings with an in-memory store, culminating in a three-component pipeline ([embedder, retriever, generator](https://www.youtube.com/watch?v=nwUhGeX6jaI)).
    - DataCamp’s Haystack agentic workflow article refactors RAG into reusable `ComponentTool`s suitable for [multi-tool agents](https://www.datacamp.com/tutorial/haystack-ai-tutorial).
2. **LlamaIndex & Local RAG**
    - The official LlamaIndex starter notebook explains agent orchestration and tool-calling, then swaps OpenAI for local Ollama-served Llama 3 in [<40 lines of code](https://docs.llamaindex.ai/en/stable/getting_started/starter_example/) ([local version](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/)).
    - Zilliz Learn’s index taxonomy (vector, tree, keyword, list) clarifies when to trade off lexical vs semantic retrieval while demonstrating persistence and [incremental updates](https://zilliz.com/learn/getting-started-with-llamaindex).
3. **Advanced Retrieval Patterns**
    - LangChain.js high-level RAG guide contrasts hybrid search, multi-vector retrieval and reranking, enabling practitioners to decide when to combine BM25 with FAISS or [`colbert-on-nx` embeddings](https://js.langchain.com/docs/concepts/rag/).
    - GoPenAI’s Haystack 2.0 deep-dive shows pipeline composition with `Pipeline.add_component`, encouraging modular replacement of [vector DB back-ends](https://blog.gopenai.com/creating-a-simple-rag-pipeline-using-haystack-2-0-c84c7c660569).

## Evaluation, Benchmarking & Safety

System‐level evaluation moves beyond single-metric accuracy to cover bias, toxicity, robustness and efficiency.

1. **Holistic Evaluation of Language Models (HELM)**
    - Stanford CRFM’s HELM paper introduces a taxonomy of sixteen scenarios and seven metrics and publishes prompt-level transparency for [30 models](https://huggingface.co/papers/2211.09110).
    - The open-source `helm` CLI reproduces leaderboards via `helm-run` and `helm-summarize`, ensuring methodological parity across [fine-tuned checkpoints](https://github.com/stanford-crfm/helm).
    - HELM Capabilities (March 2025) distils the full benchmark into five core capability scenarios for [rapid regression testing](https://crfm.stanford.edu/2025/03/20/helm-capabilities.html).
2. **lm-evaluation-harness & GuideLLM**
    - The EleutherAI harness supports >600 tasks and now exposes a template API for OpenAI-compatible endpoints, including [chat-completion log-probabilities](https://github.com/MLAI-Yonsei/lm-evaluation-harness/blob/main/docs/API_guide.md) ([and harness](https://github.com/EleutherAI/lm-evaluation-harness)).
    - YouTube’s “Strategies for LLM Evals” walkthrough demonstrates custom eval suites, human-in-the-loop scoring and [failure-mode surfacing](https://www.youtube.com/watch?v=89NuzmKokIk).
3. **Tooling Ecosystem**
    - The vLLM continuous batching benchmarks in Google Cloud’s Vertex guide quantify throughput gains vs Hugging Face TGI at [identical latency budgets](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm).
    - Sapling’s comparative analysis of FastChat vs Google Gemma surfaces dimension-specific strengths, emphasising the need to test [size-matched baselines](https://sapling.ai/llm/fastchat-vs-gemma).

## Scalable Serving & Deployment

Fine-tuned weights are only useful if they reach users with sub-second latency and manageable cost.

### 1. vLLM & PagedAttention

vLLM’s PagedAttention algorithm streams tokens from 7 B to 70 B models with constant-time KV cache manipulation, improving [GPU utilisation](https://apidog.com/blog/vllm/). Quick-start guides walk through `uv run --with vllm vllm serve` deployments in [<90 s](https://docs.vllm.ai/en/v0.8.0/getting_started/quickstart.html) ([stable version](https://docs.vllm.ai/en/stable/getting_started/quickstart.html)).

### 2. FastChat for Multi-Model Serving

FastChat provides inference, chat UI and an Arena leaderboard in [one repository](https://www.linkedin.com/pulse/understanding-fastchat-deep-dive-its-value-deployment-burmaster). PyImageSearch’s August 2024 tutorial describes Conda env setup, WebGUI launch and Q&A integration with [LangChain-runnable endpoints](https://pyimagesearch.com/2024/08/19/integrating-and-scaling-large-language-models-with-fastchat/).

### 3. Vertex AI Custom Containers

Google’s officially supported vLLM container auto-scales multi-host, multi-GPU deployments and adds LoRA dynamic loading from Cloud Storage, turning a single YAML into a [production endpoint](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm).

### 4. Local Agents with Ollama

The LlamaIndex local starter exchanges OpenAI for `ollama pull llama3.1`, demonstrating that 32 GB RAM desktops can host [8 B-parameter chat agents offline](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/).

## Professional Courses & Continuing Education

| Course / Certificate | Provider | Key Skills Covered | Format | Why Add It? |
| :-- | :-- | :-- | :-- | :-- |
| Generative AI with LLMs (AWS + DeepLearning.AI) | Coursera | Transformer internals, scaling laws, data curation, deployment pipelines | 3-module MOOC | [Bridges theory to AWS production ops](https://www.deeplearning.ai/courses/generative-ai-with-llms/) |
| Generative AI for Software Dev | DeepLearning.AI | Prompt engineering, Copilot pairing, secure dependency injection | Guided projects | [Targets full-stack engineers beyond ML niche](https://www.deeplearning.ai/courses/generative-ai-for-software-development/) |
| GANs Specialisation | DeepLearning.AI | DCGAN, StyleGAN, FID evaluation | Hands-on PyTorch labs | [Complements text LLMs with generative image skills](https://www.deeplearning.ai/courses/generative-adversarial-networks-gans-specialization/) |
| RLHF 101 Workshop | Hugging Face | UltraFeedback dataset, Armo reward model, REBEL algorithm | GitHub repo + Colab | [Practical RLHF recipes without proprietary data](https://huggingface.co/blog/GitBag/rebel) |
| Evaluation Harness Masterclass | YouTube | Custom metric hooks, agent reliability checks | 90 min video | [Operationalises model-quality dashboards](https://www.youtube.com/watch?v=89NuzmKokIk) |

All table cells fully populated with citations.

## Best-Practice Repositories & Curated Lists

- **Awesome-LLM** list tracks >600 papers, frameworks and datasets in a single README, continuously updated by [300 + contributors](https://www.kdnuggets.com/10-github-repositories-to-master-large-language-models).
- **API-Usage Best Practices** GitHub repo codifies secure key storage, exponential back-off and token-usage analytics for OpenAI, Anthropic and [Gemini APIs](https://github.com/Praveen76/LLMs-API-Usage-Best-Practices).
- **Kit Core LLM Context Guide** enumerates file-tree, symbol and semantic search strategies for [dev-oriented prompt construction](https://kit.cased.com/core-concepts/llm-context-best-practices/).


## Suggested Additions – Quick-Reference Table

| Category | Resource | Type | Value Proposition |
| :-- | :-- | :-- | :-- |
| Alignment & RLHF | [“Illustrating RLHF” – Hugging Face blog](https://huggingface.co/blog/rlhf) | Illustrated tutorial | Step-by-step visuals of policy, reward and PPO loops |
| PEFT | [Torchtune Llama-2 LoRA tutorial](https://docs.pytorch.org/torchtune/0.4/tutorials/lora_finetune.html) | Code notebook | Demonstrates LoRA gradients on consumer GPUs |
| Evaluation | [HELM Framework GitHub](https://github.com/stanford-crfm/helm) | Toolkit | Unified seven-metric benchmark suite |
| Serving | [vLLM Quick-start guide](https://docs.vllm.ai/en/v0.8.0/getting_started/quickstart.html) | Docs | PagedAttention yields 5-14× throughput gains |
| Chat Framework | [FastChat repo intro](https://www.linkedin.com/pulse/understanding-fastchat-deep-dive-its-value-deployment-burmaster) | OSS library | Single-command chat UI and benchmarking arena |
| RAG | [LangChain RAG part 1](https://python.langchain.com/docs/tutorials/rag/) | Hands-on tutorial | Builds loader-splitter-retriever-generator chain |
| Agentic RAG | [Haystack agentic workflow article](https://www.datacamp.com/tutorial/haystack-ai-tutorial) | Blog + code | Integrates custom tools into RAG pipelines |
| Instruction Tuning | [Alpaca dataset prep (WandB)](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) | Deep-dive article | Data packing, token budget optimisation |
| Best Practices | [LLM API usage repo](https://github.com/Praveen76/LLMs-API-Usage-Best-Practices) | GitHub | Production-grade error handling & key rotation |
| Professional MOOC | [Generative AI with LLMs (AWS)](https://www.deeplearning.ai/courses/generative-ai-with-llms/) | 3-week course | End-to-end lifecycle from data to deployment |

## Incorporating The Additions

1. **Create New Sections** – Add “Fine-Tuning & Alignment”, “Evaluation & Safety”, and “Serving & Deployment” headers to parallel existing categories.
2. **Inter-link Tutorials** – Where LangChain RAG is mentioned, cross-link to Haystack and LlamaIndex equivalents to accommodate user preference.
3. **Adopt a Balanced Mix** – Pair conceptual papers (HELM, RLHF) with hands-on notebooks (Torchtune, vLLM) so readers can immediately apply theory.
4. **Version Control** – Because tools evolve monthly, reference Git commits or doc versions (e.g., `vLLM 0.8.0`) to preserve reproducibility.
5. **Safety Footnotes** – Under “Responsible Use”, link HELM bias metrics and RLHF reward-hacking failure modes to augment [Cambridge guidelines](https://www.bigdatawire.com/2022/11/22/stanford-researchers-develop-helm-benchmark-for-language-models/) and [RLHF explainers](https://www.v7labs.com/blog/rlhf-reinforcement-learning-from-human-feedback).

## Conclusion

By layering these additional resources atop the existing syllabus, learners gain a cohesive pathway from mathematical foundations through hands-on fine-tuning, robust evaluation and production deployment. The roadmap deliberately blends textbooks, seminal papers, cutting-edge frameworks, video tutorials and practitioner-tested best practices, ensuring skills remain current in the rapidly evolving LLM ecosystem.

<div style="text-align: center">⁂</div>

