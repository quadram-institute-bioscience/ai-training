---
title: Ethics
---

## Responsible use of LLMs


Large Language Models (LLMs) like ChatGPT are exciting new tools for researchers, 
but using them well requires more than just typing in prompts. 

As we saw [earlier]({% post_url modules/intro/2000-01-03-limitations-of-llms %}), LLMs are not meant to generate accurate text,
and we **cannot trust** their output if used acrytically.

### Taking responsibility
<figure style="padding: 6px; font-size: 0.8em; color: #606060; background: #F0F0F0;">
    <a href="https://impact.economist.com/projects/smarter-ai-for-all/">
    <img src="{{ '/img/ibm.png' | relative_url }}" alt="IBM slide on accountability">
    </a>
    <figcaption><span style="font-size: 0.8em; color: #606060;">A computer must never take decisions</span></figcaption>
</figure>
 

This [slide](https://impact.economist.com/projects/smarter-ai-for-all/) from an IBM presentation was prepared in 1979.
It's incredibly actual: when we use LLMs we must take accountability and ownership.

But acknowledging openly the tools we used, if they played a role in generating core or original outputs (maybe not if only used for proofreading).

### Some bad examples

The availability of high-throughput text generators, unfortunately, resulted in a flood of bogus research papers. Sometimes with evident
sings of LLMs use that werent captured by the scrutiny of reviewers. This includes both **text**:

 
<figure style="padding: 6px; font-size: 0.8em; color: #606060; background: #F0F0F0;">
    <a href="https://x.com/MushtaqBilalPhD/status/1769028364092805236">
    <img src="{{ '/img/elsevier.png' |relative_url}}" alt="Nonsensical AI-generated image">
    </a>
    <figcaption><span style="font-size: 0.8em; color: #606060;">The first line of this paper includes the typical output of LLM answers.</span></figcaption>
</figure>
and even images like the infamous paper:

 
<figure style="padding: 6px; font-size: 0.8em; color: #606060; background: #F0F0F0;">
    <a href="https://www.frontiersin.org/journals/cell-and-developmental-biology/articles/10.3389/fcell.2023.1339390/full">
    <img src="{{ '/img/sperm.png' |relative_url}}" alt="Nonsensical AI-generated image">
    </a>
    <figcaption>A paper with nonsensical images generated by AI was published in a peer-reviewed journal.</figcaption>
</figure>

### Beyond the Magic Solution Myth

LLMs aren't magic wands that solve all research problems automatically. 
They're powerful tools that need careful, thoughtful use. 
Just like you wouldn't use a microscope without learning proper technique, LLMs require understanding and skill to be truly useful.

**:bulb: Key aspect**: LLMs work best when combined with human expertise and critical thinking, not as replacements for careful research.

### Three Foundation Pillars

**1. Know Your Data and Biases**

Before using any LLM, ask yourself:
* Who built this model and why?
* What data was it trained on?
* Whose voices are included—and whose are missing?

LLMs learn from existing text, which means they can amplify existing biases or dominant viewpoints while ignoring minority perspectives.

**2. Maintain Scientific Rigor**

LLMs can "hallucinate"—generate convincing but completely wrong information. This is dangerous in research where accuracy matters.

Best practices:
* Always verify LLM outputs with traditional methods
* Use domain-specific benchmarks to test results
* Document your verification process
* Embrace "slow science"—take time to check and reflect

**3. Practice Transparency**

Document the importan aspects of your use of LLMs:
* Which LLM you used and how
* What data sources informed your work
* How you verified the results
* What limitations you identified

**:bulb: Key aspect**: Transparency builds trust and enables other researchers to reproduce and build on your work.

### Your Responsibility as a Researcher

Using LLMs responsibly means:
* Maintaining healthy skepticism about AI outputs
* Being transparent about when and how you use AI tools
* Taking time to learn proper techniques
* Focusing on enhancing rather than replacing human expertise

**:arrow_right: Remember**: The goal isn't to avoid LLMs, but to use them thoughtfully as part of rigorous, ethical research practices.

### Toning down the AI hype


Since there are lots of hypes linked to LLMs, a great guide to re-scale the AI hype is a website by Carl Bergstrom and Jevin D. West:

[![Oracles]({{ '/img/bergstrom.png' | relative_url }})](https://thebullshitmachines.com/)



